{"0": {
    "doc": "Getting started",
    "title": "Getting started with cgpipe",
    "content": " ",
    "url": "/cgpipe/docs/getting-started#getting-started-with-cgpipe",
    "relUrl": "/docs/getting-started#getting-started-with-cgpipe"
  },"1": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": " ",
    "url": "/cgpipe/docs/getting-started",
    "relUrl": "/docs/getting-started"
  },"2": {
    "doc": "What is cgpipe?",
    "title": "What is cgpipe?",
    "content": "Quick version . CGPipe is a language for building data analysis pipelines. It is a declarative programming language similar to Makefiles, however instead of directly executing target scripts, jobs are submitted to a dedicated job scheduler, such as PBS, SGE, or SLURM. You define output files, which input files they need, and the script required to get from input &gt; output. Each output file (or job) is defined separately. Taken together, these job definitions form a directed acyclic graph (DAG). input &gt; file1 --&gt; file2 &gt; file3 &gt; ... &gt; output1 \\ \\-&gt; file1b &gt; output2 . or map-reduce style… . # split input input &gt; file1 + file2 + file3 # map file1 &gt; fileA file2 &gt; fileB file3 &gt; fileC #reduce fileA + fileB + fileC &gt; output . You give CGPipe these definitions and the name of the file you ultimately want. Knowing these task definitions and the desired output, CGPipe will figure out how to \"walk the graph\" to figure out what files/jobs are needed. It also backtracks the walk in case there are multiple ways to get to your output. CGPipe will look first on-disk for required input files. It will only submit a job to build a missing or out of date file. If an input is newer than an output, that output file is considered out of date. So far, this is the same as how make works, except for the job submission part… where make executes the tasks, CGPipe submits them as jobs to a scheduler. ",
    "url": "/cgpipe/docs/getting-started/what-is-cgpipe",
    "relUrl": "/docs/getting-started/what-is-cgpipe"
  },"3": {
    "doc": "What is cgpipe?",
    "title": "Similarities and differences to Makefiles",
    "content": "CGPipe pipelines have a very simliar syntax to Makefiles, particularly in the way that build-targets are defined. | Jobs are defined based on the files they will produce as output. The pipeline author can define which output files will be created, and which input files are required for a particular task. After that, a shell script snippet is included that defines the commands that need to be executed to produce the defined output files. | Jobs dependencies are automatically calculated and a build-graph produced. Jobs are then executed in the proper order along that build-graph to produce the required outputs. | CGPipe pipelines can be included in other pipelines. | If a given output file exists, it will not be rebuilt unless a defined input file will be rebuilt. CGPipe extends this to also track outputs/jobs that have been submitted to a job scheduler. If an built-target requires an input that has already been submitted to the scheduler, that input will not be resubmitted, rather the existing job will be listed as a job dependency. | . In this way, CGPipe is very similar to the qmake program that is available for SGE clusters, which executes unmodified Makefiles using the SGE scheduler. This allows for some parallelization. But there are some key differences between CGPipe pipelines and using qmake to execute an unaltered Makefile. | CGPipe allows you to specify job requirements, such as execution time, CPUs, and memory. These requirements can be set on a job or pipeline basis, allowing the pipeline author to set requirements globally or for only individual tasks. For example, the account setting can be set for all tasks, but walltime could be set on a per-task basis. | qmake runs interactively, submitting jobs in order and waiting for the results before submitting the next job. Because of this, qmake needs to keep running on the job submission host. With CGPipe, the entire pipeline is submitted to the job scheduler at once. There is no need to have a watchdog script running, as the job scheduler will automatically cancel any dependent jobs in the event of an error. | Command line arguments can be easily used to set variables within the script. | Makefiles don't include any type of flow control (if/else, for-loops), but CGPipe is a full language that includes if/else conditions and for-loops for iterating over a list of values or a range. Everything in a CGPipe line can be scripted, including target definitions. This means that an author could define a pipeline that executed in a Map-Reduce pattern where an input file is split into N number of chunks, each chunk could be processed in parallel, and then the results could be merged back together after each chunk was processed. This type of pipeline could be written in a traditional Makefile (verbosely), but by allowing build targets to be included in for-loops, the number of chunks can now be a run-time option and written in an easily readable syntax. | Build targets are scriptable with CGPipe templates . | qmake is only available for SGE clusters and isn't available for other job schedulers. CGPipe pipelines can execute on SGE or SLURM systems as well as on single hosts with SBS or bash script exports (see below). | Multiple targets can be defined for the same output files, enabling multiple execution paths to build the same output files. This means that there can be multiple set of jobs defined to yield the same output file(s), based upon what input files are available. For example, you could have paired-end next-generation sequencing reads stored in two separate FASTQ files or one interleaved FASTQ file. Either of these inputs could be used in a read alignment step to produce a BAM file, but the arguments for the alignment program may be slightly different, depending on which type of input is used. With CGPipe, you can specify two different targets for the same output file, with the targets prioritized in the order they are defined in the pipeline. If the first target (or any of its inputs) can't be used, then the next target is attempted until all possible build-graphs are exhausted. | Pipelines can be stored remotely, such as on a GitHub repository or web server. | Pipelines can be used as executable scripts using the #! (shebang) first line syntax (like bash, perl, ruby, or python scripts). | Pipelines can display help text. If the first lines are comments, then they will be displayed when help text is requested (excluding #! first lines). The first blank or non-commented line marks the end of the help text. | . ",
    "url": "/cgpipe/docs/getting-started/what-is-cgpipe#similarities-and-differences-to-makefiles",
    "relUrl": "/docs/getting-started/what-is-cgpipe#similarities-and-differences-to-makefiles"
  },"4": {
    "doc": "What is cgpipe?",
    "title": "Executing pipelines",
    "content": "It is expected that jobs will be executed on an HPC cluster with a job scheduler (SGE, SLURM, or PBS supported). This way individual tasks can be efficiently executed in a parallel manner. CGPipe will take care of setting inter-task dependencies to make sure that jobs execute in the proper order. Pipelines can also be run on a single host by using either the simple SBS scheduler or by exporting the pipeline as a bash script. SBS is well suited for single-host systems where there is no existing job scheduler. SBS requires having the sbs program installed somewhere in your $PATH. ",
    "url": "/cgpipe/docs/getting-started/what-is-cgpipe#executing-pipelines",
    "relUrl": "/docs/getting-started/what-is-cgpipe#executing-pipelines"
  },"5": {
    "doc": "Quick version",
    "title": "The quick version…",
    "content": "CGpipe is langauge for defining and building data analysis pipelines that run on an HPC cluster (for bioinformatics). The cgpipe program helps to submit jobs to the cluster and keeps track of jobs. With cgpipe, a long set of submission scripts can be reduced to a single command. But, as a language, the pipelines you write are adaptable and configurable. This means that your single command can now work for a variety of situations. So, instead of having a separate RNAseq script that is hard-coded for each project, you can now have one cgpipe script that accepts arguments for: different FASTQ files for input, different genome indexes, and any extra filtering steps. This will make your data pipelines easier to manage and more reproducible across multiple projects. ",
    "url": "/cgpipe/docs/getting-started/quick#the-quick-version",
    "relUrl": "/docs/getting-started/quick#the-quick-version"
  },"6": {
    "doc": "Quick version",
    "title": "Quick version",
    "content": " ",
    "url": "/cgpipe/docs/getting-started/quick",
    "relUrl": "/docs/getting-started/quick"
  },"7": {
    "doc": "Installation",
    "title": "Installation",
    "content": "Note: CGpipe is intended to run on a Unix/Linux workstation, server, or cluster (including macOS). CGpipe is a Java program that is packaged as either a self-executing fat JAR file, or as an embeddable library. CGpipe is a Java program, but it has been developed to run on *nix-style hosts such as Mac OSX, FreeBSD or Linux. The only pre-requisite is a working installation of Java 1.7 or better. It is untested on Windows. Step 0: Install Java . Java version 7 or higher can be used. Install Java from your Linux distribution (using apt-get or yum), or see the Oracle site for more details. Step 1: Download cgpipe . You can download the cgpipe program from the cgpipe releases website. You can save this file anywhere, but it is easiest to use if it is included on your $PATH somewhere, such as in /usr/local/bin or in a personal $HOME/bin directory. Step 2: Run a test pipeline . Most analysis pipelines you run will be custom written, however you can verify that your cgpipe installation is working with the following script: . [hello.cgp] #!/usr/bin/env cgpipe print \"Hello from cgpipe!\" . This will load cgpipe from your path and execute the above script. Right now the script doesn't do anything other than print a message to the console. If you save this as hello.cgp (and make it executable with chmod +x hello.cgp), you should see the following: . $ ./hello.cgp Hello from cgpipe! . or… . $ cgpipe hello.cgp Hello from cgpipe! . We will build from here to demonstate how to make your own pipelines in the next sections. Step 3: Configure cgpipe for your computing environment . CGpipe can run on a single user workstation, server, or HPC cluster. If you want to run more complex workflows by submitting jobs to a scheduler, it's necessary to configure cgpipe to use your scheduler. In cgpipe, job submission is handled by \"job runners\". The currently supported job schedulers are: SBS, SGE, SLURM, or PBS. For more information about available runners, or the possible configuration settings, see \"Running jobs\". If no scheduler is configured, jobs will be written as a bash script to stdout. For information about how to configure cgpipe, see: Configuring cgpipe. ",
    "url": "/cgpipe/docs/getting-started/installing",
    "relUrl": "/docs/getting-started/installing"
  },"8": {
    "doc": "Running cgpipe",
    "title": "Running cgpipe",
    "content": " ",
    "url": "/cgpipe/docs/getting-started/running",
    "relUrl": "/docs/getting-started/running"
  },"9": {
    "doc": "Running cgpipe",
    "title": "Table of contents",
    "content": ". | Using the program | Configuring cgpipe | Examples | . ",
    "url": "/cgpipe/docs/getting-started/running#table-of-contents",
    "relUrl": "/docs/getting-started/running#table-of-contents"
  },"10": {
    "doc": "Running cgpipe",
    "title": "Using the program",
    "content": "CGpipe can be run either from the command-line (cgpipe mypipeline.cgp). Or, it can be used in a pipeline script that starts with a shebang (!#) – just like any other scripting language! It is also recommended to copy cgpipe to a location in your $PATH. This way, you can use the following stub to start your script: . #!/usr/bin/env cgpipe print \"Hello pipeline!\" . In most cases, you won't be executing cgpipe directly. Instead, you'll be writing pipeline scripts in cgpipe, and those are what you'll execute. ",
    "url": "/cgpipe/docs/getting-started/running#using-the-program",
    "relUrl": "/docs/getting-started/running#using-the-program"
  },"11": {
    "doc": "Running cgpipe",
    "title": "Configuring cgpipe",
    "content": "CGpipe configuration or job variables can be set on a per-run, per-user, or per-server basis. At startup, cgpipe looks for configuration information in the following locations (in order of preference): . | From within the cgpipe JAR itself (path io/compgen/cgpipe/cgpiperc – this is only for default values), | /etc/cgpiperc (for server-level options), | $CGPIPE_HOME/.cgpiperc (CGPIPE_HOME defaults to the directory containing the cgpipe binary), | ~/.cgpiperc (user-local config), | The environmental variable CGPIPE_ENV (semi-colon delimited) | . All of these configuration files are themselves cgpipe scripts that are 'included' with your running cgpipe script. This means that they can inherit options from each other. ",
    "url": "/cgpipe/docs/getting-started/running#configuring-cgpipe",
    "relUrl": "/docs/getting-started/running#configuring-cgpipe"
  },"12": {
    "doc": "Running cgpipe",
    "title": "Examples",
    "content": ". | Do you want to set job.mail for all of your personal scripts? Then set the value in ~/.cgpiperc. | Do you want to configure the batch scheduler settings for everyone on a given system? Then set job.runner in /etc/cgpiperc. | What is cgpipe is installed as an envirnmental module? Then use $CGPIPE_HOME/.cgpiperc. | . | Do you have cgpipe installed on a network drive that is mounted on different clusters? You can still set the cgpipe runner config from $CGPIPE_HOME/.cgpiperc by setting the job.runner config values. But, you can also use if/then/endif blocks to set system specifc values. The cgpiperc files are full scripts that are interpretted, so you can make them quite customizeable. | . I've actually had to do this. We had a network drive that was shared between two different clusters. One was running CentOS 6 + PBS, the other was CentOS 7 + SGE. And, of course, the same pipelines had to be able to run on both clusters. ",
    "url": "/cgpipe/docs/getting-started/running#examples",
    "relUrl": "/docs/getting-started/running#examples"
  },"13": {
    "doc": "Language syntax",
    "title": "Language syntax",
    "content": "The cgpipe language has a simple syntax that is similar to many other languages. The flow of a script is also similar to Makefiles. The source code contains a set of test scripts that have examples of all statements and operations. These test scripts are the definitive source for the language syntax. These tests are run used to verify each build of cgpipe. In any case where this documentation conflicts with the test scripts, the test scripts are correct. Test scripts are available in the src/test-scripts directory and are named *.cgpt or *.cgpipe. ",
    "url": "/cgpipe/docs/syntax",
    "relUrl": "/docs/syntax"
  },"14": {
    "doc": "Language syntax",
    "title": "Table of contents",
    "content": ". | Contexts | Data types | Variables | Lists | Math | Logic | Variable substitution | Shell escaping | Printing | If/Else/Endif . | Conditions | . | For loops | Build target definitions . | Wildcards in targets | Target substitutions | Special targets | . | Including other files | Logging | Output logs | Comments | Help text | Job execution options . | Specifying the shell to use | Direct execution of jobs | . | Experimental cgpipe language features . | Target snippets imports | Eval statement | Double evaluated variables | . | . ",
    "url": "/cgpipe/docs/syntax#table-of-contents",
    "relUrl": "/docs/syntax#table-of-contents"
  },"15": {
    "doc": "Language syntax",
    "title": "Contexts",
    "content": "There are two contexts in a CGPipe pipeline: \"global\" and \"target\". In the \"global\" context, all uncommented lines are evaluated and treated as CGPipe code. The \"target\" context is how job commands are defined. Within a target context, the code is interpreted in \"template\" mode, where only areas wrapped in &lt;% %&gt; are evalutated as CGPipe code. The areas not wrapped in &lt;% %&gt; are treated as the body of the job script to execute. Any whitespace present in the target body is kept and not stripped. In a target, any print statements will be added to the target body, not written to the console. When a target is defined, it captures the existing global context at definition (like a closure). During execution, target contexts are therefore disconnected from the global context. In practice, this means that a target can read a global variable (if it has been set prior to the build-target definition), however, a target can not set a global variable and have the new value be visible outside or it's own context. A target is defined using the format: . output_file1 {output_file2 ... } : {input_file1 input_file2 ...} # indent to establish the target-context script body snippet &lt;% cgpipe-expression %&gt; script body snippet script body snippet &lt;% cgpipe-expression cgpipe-expression %&gt; ... # ends with outdent . ",
    "url": "/cgpipe/docs/syntax#contexts",
    "relUrl": "/docs/syntax#contexts"
  },"16": {
    "doc": "Language syntax",
    "title": "Data types",
    "content": "There are 6 primary data types in CGPipe: boolean, float, integer, list, range and string. Booleans are either true or false (case-sensitive). Strings must be enclosed in double quotes. Lists are initialized using the syntax \"[]\". Ranges can be used to iterate over a list of numbers using the syntax \"from..to\". Here are some examples: . foo = \"Hello world\" foo = 1 foo = 1.0 isvalid = true list = [] list += \"one\" list += \"two\" range = 1..10 . ",
    "url": "/cgpipe/docs/syntax#data-types",
    "relUrl": "/docs/syntax#data-types"
  },"17": {
    "doc": "Language syntax",
    "title": "Variables",
    "content": "foo = \"val\" Set a variable . foo ?= \"val\" Set a variable if it hasn't already been set . foo += \"val\" Append a value to a list (if the variable has already been set, then this will convert that variable to a list) . unset foo Unsets a variable. Note: if the variable was used by a target, it will still be set within the context of the target. Variables may also be set at the command-line like this: cgpipe -foo bar -baz 1 -baz 2. This is the same as saying: . foo = \"bar\" bar = 2.59 . ",
    "url": "/cgpipe/docs/syntax#variables",
    "relUrl": "/docs/syntax#variables"
  },"18": {
    "doc": "Language syntax",
    "title": "Lists",
    "content": "You can also create and access elements in a list using the [] splice operator. List items don't have to be of the same data type, but it is recommended that they are. List indexing starts at zero. Negative indexes are treated as relative to the end of the list. foo = [] foo = [1, 2, \"three\"] print foo[2] &gt;&gt;&gt; \"three\" print foo[-1] &gt;&gt;&gt; \"three\" . You can also append to lists: . foo = [\"foo\"] foo += \"bar\" foo += \"baz\" print foo &gt;&gt;&gt; \"foo bar baz\" . List elements can be sliced using the same [start:end] syntax as in Python. If start or end is omitted, it is assumed to be the 0 or len(list), respectively. foo = [\"one\", \"two\", \"three\"] print foo[1:] &gt;&gt;&gt; two three print foo[:2] &gt;&gt;&gt; one two print foo[:-1] &gt;&gt;&gt; one two . ",
    "url": "/cgpipe/docs/syntax#lists",
    "relUrl": "/docs/syntax#lists"
  },"19": {
    "doc": "Language syntax",
    "title": "Math",
    "content": "You can perform basic arithmetic on integer and float variables. Available operations are: . | + add | - subtract | * multiplication | / divide (integer division if on an integer) | % remainder | ** power (2**3 = 8) | . Operations are performed in standard order; however, you can also add also parentheses around clauses to process things in a different order. For example: . 8 + 2 * 10 = 28 (8 + 2) * 10 = 100 8 + (2 * 10) = 28 . ",
    "url": "/cgpipe/docs/syntax#math",
    "relUrl": "/docs/syntax#math"
  },"20": {
    "doc": "Language syntax",
    "title": "Logic",
    "content": "You can perform basic logic operations as well. This will most commonly be used in the context of an if-else condition. | &amp;&amp; and | || or | ! not (or is unset) | == equals | != not equals | &lt; less than | &lt;= less than or equals | &gt; greater than | &gt;= greater than or equals | . You can chain these together to form more complex conditions. For example: . foo = \"bar\" baz = 12 if foo == \"bar\" &amp;&amp; baz &lt; 20 print \"test\" endif . ",
    "url": "/cgpipe/docs/syntax#logic",
    "relUrl": "/docs/syntax#logic"
  },"21": {
    "doc": "Language syntax",
    "title": "Variable substitution",
    "content": "Inside of strings, variables can be substituted. Each string (including build script snippets) will be evaluated for variable substitutions. ${var} - Variable named \"var\". If \"var\" is a list, ${var} will be replaced with a space-separated string with all members of the list. **If \"var\" hasn't been set, then this will throw a ParseError exception.** ${var?} - Optional variable substitution. This is the same as above, except that if \"var\" hasn't been set, then it will be replaced with an empty string: ''. foo_@{var}_bar - A replacement list, capturing the surrounding context. For each member of list, the following will be returned: foo_one_bar, foo_two_bar, foo_three_bar, etc... foo_@{n..m}_bar - A replacement range, capturing the surrounding context. For each member of range ({n} to {m}, the following will be returned: foo_1_bar, foo_2_bar, foo_3_bar, etc... {n} and {m} may be variables or integers . ",
    "url": "/cgpipe/docs/syntax#variable-substitution",
    "relUrl": "/docs/syntax#variable-substitution"
  },"22": {
    "doc": "Language syntax",
    "title": "Shell escaping",
    "content": "You may also include the results from shell commands as well using the syntax $(command). Anything surrounded by $() will be executed in the current shell. Anything written to stdout can be captured as a variable. The shell command will be evaluated as a CGPipe string and any variables substituted. Example: . submit_host = $(hostname) submit_date = $(date) . Shell escaping can also be used within strings, such as: . print \"The current time is: $(date)\" . ",
    "url": "/cgpipe/docs/syntax#shell-escaping",
    "relUrl": "/docs/syntax#shell-escaping"
  },"23": {
    "doc": "Language syntax",
    "title": "Printing",
    "content": "You can output arbitrary messages using the \"print\" statement. The default output is stdout, but this can be silenced using the -s command-line argument. Example: . print \"Hello world\" foo = \"bar\" print \"foo${bar}\" . ",
    "url": "/cgpipe/docs/syntax#printing",
    "relUrl": "/docs/syntax#printing"
  },"24": {
    "doc": "Language syntax",
    "title": "If/Else/Endif",
    "content": "Basic syntax: . if [condition] do something... elif [condition] do something... else do something else... endif . Conditions . if foo - if the variable ${foo} was set if !foo - if the variable ${foo} was not set or is false . if foo == \"bar\" - if the variable foo equals the string \"bar\" if foo != \"bar\" - if the variable foo doesn't equal the string \"bar\" . if foo &lt; 1 if foo &lt;= 1 if foo &gt; 1 if foo &gt;= 1 . ",
    "url": "/cgpipe/docs/syntax#ifelseendif",
    "relUrl": "/docs/syntax#ifelseendif"
  },"25": {
    "doc": "Language syntax",
    "title": "For loops",
    "content": "Basic syntax: . for i in {start}..{end} do something... done for i in 1..10 do something... done for i in list do something... done . ",
    "url": "/cgpipe/docs/syntax#for-loops",
    "relUrl": "/docs/syntax#for-loops"
  },"26": {
    "doc": "Language syntax",
    "title": "Build target definitions",
    "content": "Targets are the files that you want to create. They are defined on a single line listing the outputs of the target, a colon (:), and any inputs that are needed to build the outputs. Any text (indented) after the target definition will be included in the script used to build the outputs. The indentation for the first line will be removed from all subsequent lines, in case there is a need for indentation to be maintained. The indentation can be any number of tabs or spaces. The first (non-blank) line that is at the same indentation level as the target definition line marks the end of the target definition. CGPipe expressions can also be evaluated within the target definition. These will only be evaluated if the target needs to be built and can be used to dynamically alter the build script. Any variables that are defined within the target can only be used within the target. Any global variables are captured at the point when the target is defined. Global variables may not altered within a target, but they can be reset within the context of the target itself. Example: . output1.txt.gz output2.txt.gz : input1.txt input2.txt gzip -c input1.txt &gt; output1.txt.gz gzip -c input2.txt &gt; output2.txt.gz . You may also have more than one target definition for any given output file(s). In the event that there is more than one way to build an ouput, the first listed build definition will be tried first. If the needed inputs (or dependencies) aren't available for the first definition, then the next will be tried until all methods are exhausted. In the event that a complete build tree can't be found, a ParseError will be thrown. Wildcards in targets . Using wildcards, the above could also be rewritten like this: . %.gz: % gzip -c $&lt; &gt; $&gt; . Note: The '%' is only valid as a wildcard placeholder for inputs / outputs. To use the wildcard in the body of the target, use $%. Target substitutions . In addition to global variable substitutions, within a target these additional substitutions are available. Targets may also have their own local variables. Note: For global variables, their values are captured when a target is defined. $&gt; - The list of all outputs $&gt;num - The {num}'th output (starts at 1) $&lt; - The list of all inputs $&lt;num - The {num}'th input (starts at 1) $% - The wildcard match . Special targets . There are five special target names that can be added for any pipeline: __pre__, __post__, __setup__, __teardown__, and __postsubmit__. These are target definitions that accept no input dependencies. __pre__ is automatically added to the start of the body for all targets. __post__ is automatically added to the end of the body for all targets. __setup__ and __teardown__ will always run as the first and last job in the pipeline. __postsubmit__ is a new job that is run after each other job has been submitted. There will be only one __teardown__ job for the entire pipeline, but a separate __postsubmit__ job for each other job submitted. __postsubmit__ is always a shexec block and can be used to add monitoring based on the newly submitted job-id. For example, if you'd like to keep track of jobs that were submitted, this could be used to add the new job's info (and job-id) to a database. You can selectively disable __pre__ and __post__ for any job by setting the variable job.nopre and job.nopost. ",
    "url": "/cgpipe/docs/syntax#build-target-definitions",
    "relUrl": "/docs/syntax#build-target-definitions"
  },"27": {
    "doc": "Language syntax",
    "title": "Including other files",
    "content": "Other Pipeline files can be imported into the currently running Pipeline by using the include filename statement. In this case, the directory of the current Pipeline file will be searched for 'filename'. If it isn't found, then the current working directory will be searched. If it still isn't found, then an ParseError will be thrown. ",
    "url": "/cgpipe/docs/syntax#including-other-files",
    "relUrl": "/docs/syntax#including-other-files"
  },"28": {
    "doc": "Language syntax",
    "title": "Logging",
    "content": "You can define a log file to use within the Pipeline file. You can do this with the log filename directive. If an existing log file is active, then it will be closed and the new log file used. By default all output from the Pipeline will be written to the last log file specified. You may also specify a log file from the command-line with the -l logfile command-line argument. ",
    "url": "/cgpipe/docs/syntax#logging",
    "relUrl": "/docs/syntax#logging"
  },"29": {
    "doc": "Language syntax",
    "title": "Output logs",
    "content": "You can keep track of which files are scheduled to be created using an output log. Do use this, you'll need to set the cgpipe.joblog variable. If you set a joblog, then in addition to checking the local filesystem to see if a target already exists, the joblog will also be consulted. This file keeps track of outputs that have already been submitted to the job scheduler. CGPipe will also check with the job runner, to verify that the job is still valid (running or queued). This way you can avoid re-submitting the same jobs over and over again if you re-run the pipeline. This output log enables the ability to have multiple pipelines coordinate common dependencies or chaining without requiring an external management daemon. This also allows you to write smaller separate (composable) pipelines instead of large monolithic ones. ",
    "url": "/cgpipe/docs/syntax#output-logs",
    "relUrl": "/docs/syntax#output-logs"
  },"30": {
    "doc": "Language syntax",
    "title": "Comments",
    "content": "Comments are started with a # character. You may also include the '$' and '@' characters in strings or evaluated lines by escaping them with a '' character before them, such as \\$. If they will be evaluated twice, you will need to escape them twice (as is the case with shell evaluated strings). ",
    "url": "/cgpipe/docs/syntax#comments",
    "relUrl": "/docs/syntax#comments"
  },"31": {
    "doc": "Language syntax",
    "title": "Help text",
    "content": "The user can request to disply help/usage text for any given pipeline. Any comment lines at the start of the file will be used as the help/usage text. The first non-comment line (including blank lines) will terminate the help text. If the script starts with a shebang (#!), then that line will not be included in the help text. Example: . #!/usr/bin/env cgpipe # # This is a pipeline # # Options: # --gzip compress output # --input filename input filename # # (end of the help text) ... # rest of the script . ",
    "url": "/cgpipe/docs/syntax#help-text",
    "relUrl": "/docs/syntax#help-text"
  },"32": {
    "doc": "Language syntax",
    "title": "Job execution options",
    "content": "Specifying the shell to use . CGPipe will attempt to find the correct shell interpreter to use for executing scripts. By default it will look for /bin/bash, /usr/bin/bash, /usr/local/bin/bash, or /bin/sh (in order of preference). Alternatively, you may set the config value cgpipe.shell in the $HOME/.cgpiperc file to set a specific shell binary. The shell may also be chosen on a per-job basis by setting the job.shell variable for each job. Direct execution of jobs . Certain jobs can also be directly executed as part of the pipeline building process. Instead of submitting these jobs to a scheduler, the jobs can be put into a temporary shell script and executed directly. The global shell will be used to run the script. Only jobs without any dependencies can be executed in this manner. If you would like a job to just run directly without being scheduled, set the variable job.shexec=true. Also, the __setup__ and __teardown__ can be executed as shexec. One use for this is to setup any output folders that may be required. For example: . __setup__: &lt;% job.shexec = true %&gt; mkdir -p output . Another common use-case for this is having a clean target to remove all output files to perform a fresh set of calculations. For example: . clean: &lt;% job.shexec = true %&gt; rm *.bam . ",
    "url": "/cgpipe/docs/syntax#job-execution-options",
    "relUrl": "/docs/syntax#job-execution-options"
  },"33": {
    "doc": "Language syntax",
    "title": "Experimental cgpipe language features",
    "content": "The following features are experimental. Syntax for the below may change in future versions of CGPipe (or be removed entirely). ",
    "url": "/cgpipe/docs/syntax#experimental-cgpipe-language-features",
    "relUrl": "/docs/syntax#experimental-cgpipe-language-features"
  },"34": {
    "doc": "Language syntax",
    "title": "Target snippets imports",
    "content": "Sometimes you might have more than one target definition that has the same (or similar) job body. In this case, you might want to have only one copy of the source snippet and import that copy into each separate build-target script. You can do this with an \"importable\" target definition. This is one way to include a common snippet into a target script that isn't __pre__ or __post__. Importable target definitions are targets that have only one output (the name), followed by two colons. That snippet can then be imported into the body of a target definition using the import statement. (Note: the import statement only works within the context of a build-target. If you need something like import in a Pipeline, try the include statement.) . Here's an example: . common:: echo \"this is the common snippet\" out.txt: input.txt &lt;% import common %&gt; out2.txt: input2.txt &lt;% import common %&gt; . ",
    "url": "/cgpipe/docs/syntax#target-snippets-imports",
    "relUrl": "/docs/syntax#target-snippets-imports"
  },"35": {
    "doc": "Language syntax",
    "title": "Eval statement",
    "content": "The eval statment lets us eval a string at runtime . i=1 a=\"i=i+1\" eval a i =&gt; 2 . ",
    "url": "/cgpipe/docs/syntax#eval-statement",
    "relUrl": "/docs/syntax#eval-statement"
  },"36": {
    "doc": "Language syntax",
    "title": "Double evaluated variables",
    "content": "Double var eval is useful in targets to include chunks of text based on a var: . foo=\"echo \\\"$&gt;\\\"\" target: $ . will result in this being added to the body: . echo \"target\" . ",
    "url": "/cgpipe/docs/syntax#double-evaluated-variables",
    "relUrl": "/docs/syntax#double-evaluated-variables"
  },"37": {
    "doc": "Job scheduling",
    "title": "Pipeline backends (batch schedulers)",
    "content": " ",
    "url": "/cgpipe/docs/running-jobs#pipeline-backends-batch-schedulers",
    "relUrl": "/docs/running-jobs#pipeline-backends-batch-schedulers"
  },"38": {
    "doc": "Job scheduling",
    "title": "Table of contents",
    "content": ". | HPC server backends | Specifying requirements | Runner specific configuration . | Template scripts | Shell script export | Simple Batch Scheduler (SBS) | PBS (Torque/PBS) | SGE/OGE | SLURM | . | Dry-runs | Job pipeline graph | . Unlike Make, cgpipe does not actually execute pipeline commands. Instead, running the commands is outsourced to a \"job runner\" or backend. There are a number of supported backends, covering some of the more common HPC schedulers. There are curently 5 available backends for running pipelines: a combined shell script (default), SGE/Open Grid Engine, PBS, SLURM, and single-user SBS (also from compgen.io, see below). Job runners are chosen by setting the configuration value cgpipe.runner in $HOME/.cgpiperc to either: 'pbs', 'sge', 'slurm', 'sbs', or 'shell' (default). Example: . cgpipe.runner=\"sbs\" cgpipe.runner.sbs.sbshome=\"/home/username/.sbs\" . Like other cgpipe-wide configuration variables, any of these values can be changed on a per-run, per-user, or per-server basis. ",
    "url": "/cgpipe/docs/running-jobs#table-of-contents",
    "relUrl": "/docs/running-jobs#table-of-contents"
  },"39": {
    "doc": "Job scheduling",
    "title": "HPC server backends",
    "content": "The most common use-case for CGPipe is running jobs within an HPC context. Currently, the only HPC job schedulers that are supported are SGE/Open Grid Engine and SLURM. CGPipe integrates with these schedulers by dynamically generating job scripts and submitting them to the scheduler by running scheduler-specific programs (qsub/sbatch). ",
    "url": "/cgpipe/docs/running-jobs#hpc-server-backends",
    "relUrl": "/docs/running-jobs#hpc-server-backends"
  },"40": {
    "doc": "Job scheduling",
    "title": "Specifying requirements",
    "content": "Resource requirements for each job (output-target) can be set on a per-job basis by setting CGPipe variables. Because of the way that variable scoping works, you can set any of the variables below at the script or job level. | job setting | description | shell | sge | slurm | pbs | sbs | . | job.name | Name of the job |   | X | X | X | X | . | job.procs | Number of CPUs (per node) |   | X | X | X | X | . | job.walltime | Max wall time for the job |   | X | X | X |   | . | job.mem | Req'd RAM (ex: 2M, 4G) [*] |   | X | X | X | X | . | job.stack | Req'd stack space (ex: 10M) |   | X |   |   |   | . | job.hold | Place a user-hold on the job |   | X | X | X | X | . | job.env (T/F) | Capture the current ENV vars |   | X | X | X |   | . | job.qos | QoS setting |   |   | X | X |   | . | job.project | Project setting |   | X |   |   |   | . | job.priority | Priority setting |   | X |   |   |   | . | job.nice | Job \"nice\" setting |   |   |   | X |   | . | job.queue | Specific queue to submit job to |   |   |   | X |   | . | job.wd | Working directory |   | X | X | X | X | . | job.account | Billing account |   | X | X | X |   | . | job.mail | Mail job status |   | X | X | X | X | . | job.mailtype | When to send mail |   | [1] | [2] | X |   | . | job.src | Write the submitted script to a file |   | X | X | X | X | . | job.stdout | Capture stdout to file |   | X | X | X | X | . | job.stderr | Capture stderr to file |   | X | X | X | X | . | job.shell | Job-specific shell binary | [3] | X | X | X |   | . | job.node.property | Property requirement for an exec node |   |   |   | X |   | . | job.node.hostname | Exact host to run job on |   |   |   | X |   | . | global job setting | description |   |   |   |   |   | . | job.shexec(T/F) | Exec job; don't submit job | X | X | X | X | X | . | job.nopre (T/F) | Don't include global pre | [4] | X | X | X | X | . | job.nopost(T/F) | Don't include global post | [4] | X | X | X | X | . [*] Memory should be specified as the total amount required for the job, if necessary, cgpipe will re-calculate the per-processor required memory. [1], [2] - job.mailtype has slightly different meanings for SGE and SLURM. The possible values are different for each scheduler. [3] - the shell for the shell runner can be set using the global shell config, but the defaults are \"/bin/bash\", \"/usr/bin/bash\", \"/usr/local/bin/bash\", \"/bin/sh\" (in that order of priority). [4] - pre and post script are only included once for the shell runner, so if any job includes a pre or post, then the final script will as well. ",
    "url": "/cgpipe/docs/running-jobs#specifying-requirements",
    "relUrl": "/docs/running-jobs#specifying-requirements"
  },"41": {
    "doc": "Job scheduling",
    "title": "Runner specific configuration",
    "content": "Runner configurations may be set using the form: cgpipe.runner.{runner_name}.{option}. Each runner has different options, which are listed below. For PBS, SGE, SLURM, and SBS, you have the option: cgpipe.runner.{runner_name}.global_hold. If global_hold is true, then each job will have a user-hold placed on it until the entire pipeline has been submitted. Once the entire pipeline has been submitted (successfully), the user-hold will be released and the pipeline can start. This is useful to make sure that any step of the pipeline will run if and only if the entire pipeline was able to be submitted. This also makes sure that quick running jobs don't finish before their child jobs have been submitted. If there is an issue with submitting any of the jobs, then all jobs will be aborted before they are released for execution. For PBS, SGE, and SLURM, you can also set a global default account by using the cgpipe.runner.{runner_name}.account option. Template scripts . PBS, SGE, SLURM, and SBS job runners all operate by processing a job template, setting the appropriate variables, and then calling the appropriate executable to submit the job (qsub, sbatch, or sbs). A basic job template is included in CGPipe for each of these schedulers; however, if you'd like to use your own template, this can be specified by setting the variable cgpipe.runner.{runner_name}.template. As with all other options, this can be done on an adhoc, per-user or per-host basis. If you'd like to write your own templates, the templates are themselves written as CGPipe scripts and can include logic and flow-control. | PBSTemplateRunner.template.cgp | SBSTemplateRunner.template.cgp | SGETemplateRunner.template.cgp | SLURMTemplateRunner.template.cgp | . Shell script export . Shell scripts will write a single script that contains all of the tasks for a given pipeline as functions in the script. When the script is executed, each of the functions will be executed in order. In case multiple pipelines need to be run, the variable cgpipe.runner.shell.filename can be set. If this is set, then each successive pipeline will be added to this single script file. CGPipe will manage the appropriate job names to avoid collisions. Additionally, if you write the output to a file, the presence of each output will be checked by the script. This means that only if an output file is missing will a job be executed. By default the script is written to stdout. The shell runner has one specific option that can be set: cgpipe.runner.shell.autoexec. If this is set, then instead of writing the assembled shell script to stdout, the script will be immediately executed. Simple Batch Scheduler (SBS) . https://github.com/compgen-io/sbs . SBS has two other options (cgpipe.runner.sbs.): sbshome and path. sbshome sets where the SBS job scripts will be tracked. By default this is in the current directory under .sbs. However, this can be set by the $SBSHOME environmental variable or overridden by this property. path is the path to the sbs program, if it isn't part of your $PATH. PBS (Torque/PBS) . PBS has three unique options (cgpipe.runner.pbs.): trim_jobid, use_vmem and ignore_mem. trim_jobid will trim away the cluster name from the jobid returned from qsub. If your cluster returns something like \"1234.cluster.hostname.org\" from qsub, but requires \"1234\" for qstat, etc… then this option will trim away the \"cluster.hostname.org\" from the captured jobid. use_vmem will set all memory restrictions with -l vmem=XX as opposed to the default -l mem=XX. And ignore_mem will ignore any memory restrictions whatsoever in the job submission script (this may cause problems with your cluster or job scheduler, so use at your own risk). SGE/OGE . For SGE, there are two additional options (cgpipe.runner.sge.): the name of the parallel environment needed to request more than one slot per node (parallelenv; -pe in qsub), and if the memory required should be specified per job or per slot (hvmem_total; -l h_vmem in qsub). The default parallelenv is named 'smp' and by default h_vmem is specified on a per-slot basis (hvmem_total=F). SLURM . SLURM has no additional options that haven't already been mentioned above. ",
    "url": "/cgpipe/docs/running-jobs#runner-specific-configuration",
    "relUrl": "/docs/running-jobs#runner-specific-configuration"
  },"42": {
    "doc": "Job scheduling",
    "title": "Dry-runs",
    "content": "If you want to see the script that would be sent to the job scheduler, you can specify both the verbose (-v) and dry-run (-dr) options to cgpipe and the job scripts that would be submitted will be printed to stdout as opposed to being sent to the job scheduler. This is a good way to troubleshoot parameters and custom templates if needed. These options can be sent to any cgpipe command, or if a script uses the #!cgpipe or #!/usr/bin/env cgpipe shebang syntax, these options can be provided directly to that script. Options with a single dash (-) are sent to cgpipe itself whereas options with a double dash (--) are used as argument to your pipeline. The shell job-runner is also very useful for troubleshooting pipelines, so that one can verify if the job recipe scripts are correct. ",
    "url": "/cgpipe/docs/running-jobs#dry-runs",
    "relUrl": "/docs/running-jobs#dry-runs"
  },"43": {
    "doc": "Job scheduling",
    "title": "Job pipeline graph",
    "content": "An included non-executing job runner is also available that converts the job dependency graph into a graphviz model. To use this runner, you need to set cgpipe.runner=\"graphviz\". Instead of executing the job tree, this will produce a graphviz dot file to show the dependencies between jobs. This can be used to generate a PDF or PNG image using the Graphviz tools. This does not currently support displaying multiple interconnected pipelines through the the joblog, but that is a possibility for a future version. ",
    "url": "/cgpipe/docs/running-jobs#job-pipeline-graph",
    "relUrl": "/docs/running-jobs#job-pipeline-graph"
  },"44": {
    "doc": "Job scheduling",
    "title": "Job scheduling",
    "content": " ",
    "url": "/cgpipe/docs/running-jobs",
    "relUrl": "/docs/running-jobs"
  },"45": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "The easiest way to get started with writing CGPipe pipelines is to see a few examples. We'll start with a few easy examples and then work up to some fully fleshed out examples (that are in production use). ",
    "url": "/cgpipe/docs/tutorials",
    "relUrl": "/docs/tutorials"
  },"46": {
    "doc": "Tutorial: Flexible pipelines",
    "title": "Tutorial: Flexible pipelines",
    "content": " ",
    "url": "/cgpipe/tutorials/flexible-pipelines",
    "relUrl": "/tutorials/flexible-pipelines"
  },"47": {
    "doc": "Tutorial: Flexible pipelines",
    "title": "Overview",
    "content": ". | Rationale | . ",
    "url": "/cgpipe/tutorials/flexible-pipelines#overview",
    "relUrl": "/tutorials/flexible-pipelines#overview"
  },"48": {
    "doc": "Tutorial: Flexible pipelines",
    "title": "Rationale",
    "content": "One of the main reasons to use CGPipe … . ",
    "url": "/cgpipe/tutorials/flexible-pipelines#rationale",
    "relUrl": "/tutorials/flexible-pipelines#rationale"
  },"49": {
    "doc": "Job log",
    "title": "Job log",
    "content": "This job log is a feature that helps to coordinate different pipelines running on the same dataset. Fundamentally, the job-log is a text file that contains a list of each job that was submitted to the scheduler for a given project. ",
    "url": "/cgpipe/docs/features/job-log",
    "relUrl": "/docs/features/job-log"
  },"50": {
    "doc": "Job log",
    "title": "Use case",
    "content": "For example, you may want to split your analysis into different sections. For example, . | aligning FASTQ data to a genome | post-processing the data | calling variants | . These are three separate pipelines could be completely separate scripts. However, for a given project, it might make sense to process them in this order. Without using a job-log, you have two main options: . | Combine all the steps into one monolithic pipeline, or | Run each step separately, waiting until the first was done to execute the second, etc… | . One problem with the first approach is that you now have to manage each step in one large pipeline – one for each project. But what happens if you want to change one of the steps? Let's say you want to change the alignment program… now you need to change this pipeline code in each monolithic pipeline, for each project. This would lead to a lot of repeated effort. The second approach fixes the first issue – repeated code – but at the expense of efficiency. When you have all of steps in a monolithic list, each job submitted to the scheduler at the same time. This means that the jobs can be scheduled as efficiently as possible. If you have to wait for multiple steps to finish before submitting jobs, you could end up with idle cluster time, or delayed priorities for submitted jobs. Note: there is a third option, where you create pipeline steps like libraries or functions. Then you could effectively import the steps you need, but this is very heavily dependent upon the pipeline manager. CGpipe does support this method, but using the job-log is still recommended. ",
    "url": "/cgpipe/docs/features/job-log#use-case",
    "relUrl": "/docs/features/job-log#use-case"
  },"51": {
    "doc": "Remote pipelines",
    "title": "Remote pipelines",
    "content": "Note: this is an experimental feature . Pipeline files don't have to be on the local filesystem in order to be run. Remote pipelines can be used anywhere that a local pathname could be used for CGPipe. Remote pipelines can either be given as explicit HTTP or HTTPS URLs or use a \"named remote\". Named remotes are web addresses that can be accessed using a shortcut naming scheme to make it easier to use interactively. ",
    "url": "/cgpipe/docs/features/remote-pipelines",
    "relUrl": "/docs/features/remote-pipelines"
  },"52": {
    "doc": "Remote pipelines",
    "title": "Defining custom named remote sources",
    "content": "Defining a new named remote source can be accomplished by setting a new variable in CGPipe (either from a pipeline or from a global RC script). The new value should be cgpipe.remote.$shortname$.baseurl. The value should be the base-url to use for the resource. As an example: . cgpipe.remote.compgen_io.baseurl = \"https://raw.githubusercontent.com/compgen-io/cgpipe-pipelines/master/\" . You can then load a remote pipeline using the remote-name:filename syntax. As an example, to see the help text for the pipeline compgen_io:pipelines, you'd be able to run the following: . cgpipe -h -f compgen_io:pipelines . where the pipeline script itself is loaded from https://raw.githubusercontent.com/compgen-io/cgpipe-pipelines/master/pipelines. ",
    "url": "/cgpipe/docs/features/remote-pipelines#defining-custom-named-remote-sources",
    "relUrl": "/docs/features/remote-pipelines#defining-custom-named-remote-sources"
  },"53": {
    "doc": "Remote pipelines",
    "title": "SHA-1 hashes",
    "content": "Because remote pipelines can be updated at any time, it is important to be able to track if a pipeline is what you expect it to be. This can either mean logging the SHA-1 hash of the script, actively verifying the SHA-1 hash of a local or remote pipeline. The current filename and hash is available with the special variables cgpipe.current.filename and cgpipe.current.hash. Note: If tracking the versions of pipelines is critical (as in version-controlled pipelines), then it is recommended that any remote pipelines be located on a server controlled by you and that each file hash is verified as the pipeline is loaded. You can verify any local or remote pipelines by including the expected SHA-1 hash in the filename as shown below. From within the pipeline: . `include remote/filename#expected-sha1-hash` . or using the cgpipe executable syntax: . `cgpipe -f \"localfile#sha1-hash\"` (Note the quotes - otherwise the `#` would be interpreted as a comment) . ",
    "url": "/cgpipe/docs/features/remote-pipelines#sha-1-hashes",
    "relUrl": "/docs/features/remote-pipelines#sha-1-hashes"
  },"54": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Tutorial: Docker with cgpipe",
    "content": " ",
    "url": "/cgpipe/tutorials/docker-cgpipe",
    "relUrl": "/tutorials/docker-cgpipe"
  },"55": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Overview",
    "content": ". | Heredocs | Docker . | Job integration | Alternatives | . | Meta-targets | Job settings | Putting it all together | . CGpipe is a powerful tool for executing bioinformatics data analysis workflows. Much of its power derives from its inherent flexibility. In addition to supporting a variety of batch scheduling environments (SGE, SLURM, PBS), it can support different modes of executing jobs, such as in the context of a Docker container. The method we'll use to allow jobs to run inside a container is to use the HEREDOC pattern along with some generic features of cgpipe. ",
    "url": "/cgpipe/tutorials/docker-cgpipe#overview",
    "relUrl": "/tutorials/docker-cgpipe#overview"
  },"56": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Heredocs",
    "content": "What are heredocs? Heredocs are a feature of shell scripts that let you send multiple lines of data as stdin to a program (an input-stream literal). One common way they are used is to write multiple lines to a file at a time. cat &lt;&lt;HEREDOC &gt; output.txt line 1 line 2 etc... HEREDOC . You don't need to use the delimiter HEREDOC, it could be any string. Another commonly used string is EOF. cat &lt;&lt;EOF &gt; output.txt etc... EOF . We are going to use this to have CGpipe wrap a job script and send that as stdin to bash running in a container. ",
    "url": "/cgpipe/tutorials/docker-cgpipe#heredocs",
    "relUrl": "/tutorials/docker-cgpipe#heredocs"
  },"57": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Docker",
    "content": "We'll assume you're already familiar with Docker, but briefly Docker a platform for running programs within an isolated container. The container lets you build and ship programs with any needed external dependencies already in place. This results in a consistent execution environment for data analysis pipelines and can eliminate the issue of installing complex dependencies for individual programs. It is out of the scope of this document to go into all of the ways that Docker can be started (or how to create a container image). However, if your job can be started with a shell script, you can use this method to run your job in a container. We will use docker run to start our script. Job integration . In order to use the Docker container, we will mount the current working directory as /data in the Docker container. This will be the working directory for the job. We will then start the container and execute the given job snippet in the context of the container by using /bin/bash as our Docker entrypoint. Note: This is only one way to do this and may not be the best solution for your environment. But this should be a good starting point to adapt to your system. Alternatives . Here are some alternatives to Docker containers for software versioning: . | Environment modules - The original method for managing program versions. This (or a similar tool) is installed on most HPC clusters. | Singularity containers - Docker-alternative container tool from Berkeley Lab that doesn't require special permissions and is compatible with existing multi-user HPC systems. The technique used here for Docker should also be applicable to Singularity containers. | . ",
    "url": "/cgpipe/tutorials/docker-cgpipe#docker",
    "relUrl": "/tutorials/docker-cgpipe#docker"
  },"58": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Meta-targets",
    "content": "The feature of CGPipe that will help us get our jobs running in a container are the setup/teardown/pre/post meta-targets. These are named __setup__, __teardown__, __pre__, and __post__ and are defined like any other job definition. However, setup and teardown are executed at pipeline run-time (not submitted to a scheduler) to do things like make a child directory or remove a temporary file. Setup is run before any other jobs are submitted to the scheduler and teardown is executed before CGPipe returns. However, pre and post are job meta-targets that are included in the main job snippet. Crucially, they are not job dependencies, they are included in the body of the job script itself. How can this be helpful? One example is to track the time required for a job to start/finish. Let's say you had the following job definition: . output.txt: input.txt ./myprog input.txt &gt; output.txt . This would result in the following job script: . #!/bin/bash ./myprog input.txt &gt; output.txt . As an example, this is a pretty simple pipeline that has only one task. If you wanted to write the start / finish times to stdout for this job, you could use the pre and post meta-targets like this: . __pre__: echo \"Start: \\\\$(date)\" __post__: echo \"Done: \\\\$(date)\" output.txt: input.txt ./myprog input.txt &gt; output.txt . This will result in a job script that looks something like this: . #!/bin/bash echo \"Start: $(date)\" ./myprog input.txt &gt; output.txt echo \"Done: $(date)\" . If you have multiple tasks, pre and post are applied to each job. This makes these meta targets a powerful method for altering or monitoring the execution of each task in a pipeline. Some other examples where the pre and post meta-targets could come in handy tracking job start/finish in an external monitoring program, downloading inputs from cloud storage (e.g. S3), uploading outputs to cloud storage, changing file permissions, etc… . Note the escaped \\\\$(date) above – if this wasn't escaped CGPipe would shell out to get the date at run-time. This would then show the date/time that the script was run. An alternative would be to avoid the shell escape and do something like this: . __pre__: echo -n \"Start: \" date . ",
    "url": "/cgpipe/tutorials/docker-cgpipe#meta-targets",
    "relUrl": "/tutorials/docker-cgpipe#meta-targets"
  },"59": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Job settings",
    "content": "CGPipe job snippets are nothing more than shell (bash) scripts, which in combination with the pre and post meta-targets gives a pipeline author a lot of flexibility. It also means that you can adapt the pipeline to work with your system, not adapt your system to the pipeline. Because not all jobs will require a Docker container to execute, it is appropriate to configure the container settings on a job-by-job basis. Like all job settings, it is possible to set these on a per-job, per-pipeline, or per-system basis. Any cgpipe variable starting with job. (including user-defined ones) is available within the job snippet context (including pre and post), so we will use that to set a few Docker-specific variables. Specifically, we will use job.docker.container. job.docker.container will be set to the container image name to use for this job. You can also use a similar strategy to set things like volumes or other resource requirements for the container, but we'll keep things simple for now. ",
    "url": "/cgpipe/tutorials/docker-cgpipe#job-settings",
    "relUrl": "/tutorials/docker-cgpipe#job-settings"
  },"60": {
    "doc": "Tutorial: Docker with cgpipe",
    "title": "Putting it all together",
    "content": "For an example, lets assume you have a BAM file and you'd like to calculate some alignment QC statistics. One way to do this would be to use ngsutilsj and run the following: . ngsutilsj bam-stats input.bam &gt; output.stats.txt . We may not have ngsutilsj installed on our system, but it is available in the asclab/spcg-working Docker container. To setup the Docker container, we'll use the pre and post meta-targets to wrap the entire job snippet in a HEREDOC. That will then send the script to a the container's entrypoint (/bin/bash) to execute in the context of the container. Here is what our trival example looks like as a cgpipe script: . #!/usr/bin/env cgpipe output.stats.txt: input.bam ngsutilsj bam-stats input.bam &gt; output.stats.txt . Now, we can add the __pre__ and __post__ to wrap the snippet. Note: this assumes the program realpath is installed. realpath is used to get the absolute path for the current directory. Absolute paths are required by Docker to bind local directories as volumes in the container (you can also get this with a Perl or Python one-liner). #!/usr/bin/env cgpipe __pre__: &lt;% if job.docker.container %&gt; docker run --rm -i \\ -v $(realpath .):/data \\ -w /data ${job.docker.container} /bin/bash &lt;&lt;HEREDOC &lt;% endif %&gt; __post__: &lt;% if job.docker.container %&gt; HEREDOC &lt;% endif %&gt; output.stats.txt: input.bam &lt;% job.docker.container=\"asclab/spcg-working\" %&gt; ngsutilsj bam-stats input.bam &gt; output.stats.txt . This results in the following job script: . #!/bin/bash docker run --rm -i \\ -v /my/path:/data \\ -w /data asclab/spcg-working /bin/bash &lt;&lt;HEREDOC ngsutilsj bam-stats input.bam &gt; output.stats.txt HEREDOC . One thing to look out for is the whitespace surrounding the HEREDOC in __post__. This needs to end up as the first thing on a line without any preceeding whitespace. If you use consistent indentation, this isn't a problem, but it's something to look out for when troubleshooting. ",
    "url": "/cgpipe/tutorials/docker-cgpipe#putting-it-all-together",
    "relUrl": "/tutorials/docker-cgpipe#putting-it-all-together"
  },"61": {
    "doc": "Features",
    "title": "Features",
    "content": "Here are some of the features that makes cgpipe unique and helps to scale analysis pipelines. ",
    "url": "/cgpipe/docs/features",
    "relUrl": "/docs/features"
  },"62": {
    "doc": "Projects: cgpipe",
    "title": "cgpipe – language for bioinformatics pipelines",
    "content": "CGpipe is a bioinformatics pipeline development language. It is very similar in spirit (and practice) to the venerable Makefile, but geared towards execution in an HPC environment. Like Makefiles, cgpipe pipelines establish a series of build targets, their required dependencies, and a recipe (shell script) used to build the outputs. Unlike Makefiles, cgpipe pipelines are executable scripts that can incorporate logic and flow control in the setup and execution of the build-graph. Additionally, cgpipe doesn't actually execute your job scripts – instead, it submits these jobs to a proper job scheduler for efficient distributed processing across a computational cluster. CGpipe is distributed as a self-executing fat-JAR file. This means that for installation, all one needs is a working copy of Java and the cgpipe file. ",
    "url": "/cgpipe/#cgpipe--language-for-bioinformatics-pipelines",
    "relUrl": "/#cgpipe--language-for-bioinformatics-pipelines"
  },"63": {
    "doc": "Projects: cgpipe",
    "title": "History of cgpipe",
    "content": "CGpipe was written to support managing computational workflows in the study of pediatric cancers at Stanford University and the University of California San Francisco. For each patient sample sequenced, hundreds of jobs are generated and processed. This required the use of many different high-performance computing clusters over the years, including PBS, SGE, and SLURM clusters. The difficulty in migrating pipelines and adapting them to the different systems was one of the main motivations for cgpipe. However, the major motivation was the desire to the the writing and use of complex pipelines as comparable to writing a simple shell script. For example, processing a single RNAseq sample may require 10's of jobs, but most of the logic is consistent between runs. There are a few variables (which FASTQ files to use, which reference genome, etc…), but for the most part, these can be handled with simple command line arguments. Then question then became – how can we make something that works like this: . $ ./rnaseq --fq1 input_R1.fastq --fq2 input_R2.fastq --ref GRCh38.fa --output sample1/ . CGpipe was written to support this type of workflow. With this as the model, cgpipe has been refined over the years and has processed hundreds of thousands of jobs. If this is something that you need for data pipeline work, give it a try. If you have any questions or find any issues with cgpipe, please contact me! (Or submit an issue on github!) . ",
    "url": "/cgpipe/#history-of-cgpipe",
    "relUrl": "/#history-of-cgpipe"
  },"64": {
    "doc": "Projects: cgpipe",
    "title": "Projects: cgpipe",
    "content": " ",
    "url": "/cgpipe/",
    "relUrl": "/"
  }
}
